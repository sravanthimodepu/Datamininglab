

<center>
<img src="http://virtual-labs.ac.in/images/virtualLabsLogo.jpg" width="1220" alt="logo"/> 
</center>
<h1 style="color: green;">The Partition Algorithm</h1>
<h2 style="color: green;">Introduction</h2>
<p>
The Partition algorithm, published by Navathe <i>et al</i> in VLDB 1995, was a very elegant algorithm designed to alleviate the first bottle-neck of Apriori, which is the large number of database scans. Partition only requires two disk-based scans of the data to perform mining. This algorithm first partitions the dataset horizontally. Each partition is chosen to be small enough to fit well into main-memory. Note that this partitioning is only logical â€“ the data on disk is not touched during this process.</p>

<h2 style="color: green;">Objective</h2>
<p>
Through this experiment we will demonstrate the working of the Partition algorithm, which is designed to operate in only two scans of the database.
</p>

<h2 style="color: green;">Theory</h2>

<h3>Problem Formulation:</h3>

<p>The problem formulation is the same as that of Apriori, except that it also requires an additional parameter as input -- which is the size of each partition. The partitions have to be as large as possible, with the constraint that each one has to fit into main-memory along with intermediate results of the mining operation.</p>

<h3>Algorithm:</h3>
<p>The algorithm proceeds in the following manner:</p>

<p><b>Scan 1:</b> During the first scan over the dataset, each partition is loaded into main-memory and an Apriori-type mining (in main-memory) is carried out on it. The frequent itemsets local to each partition are pooled together to form a global candidate itemset collection.</p>

<p><b>Scan 2:</b> Finally, the candidates pooled in scan 1 are counted over the entire dataset during a second scan and then the globally frequent itemsets are output.</p>

<p>The key observation that makes the Partition algorithm possible is the following theorem:</p>

<p><b>Theorem:</b> An itemset can be globally frequent only if it is locally frequent in at least one partition.</p>

<b>Proof:</b> Let $X$ be an itemset that is infrequent locally in all partitions. We will show that $X$ cannot be frequent globally. Let $P_i$ be the $i^{th}$ partition and its size be $|P_i|$ records. Let $c_i$ be the count of $X$ in $P_i$ and $c$ be its count over the entire dataset $D$. Because $X$ is infrequent in $P_i$:
$$c_i < minsup \times |P_i|$$
Summing this over all partitions, we get:
$$c < minsup \times |D|$$
Hence $X$ is infrequent globally.

<h2 style="color: green;">Procedure</h2>

<br />
We present the Partition simulation in the next section. Some points to note are as follows:<br />

<ol>
<li>
The minimum support for the experiment is 0.6
</li>
<li> Clicking on 'evaluate' will run the algorithm.</li>
</ol>

<h2 style="color: green;">Simulation</h2>

Consider the following dataset:<br /><br /><br />

{{{id=1|
#auto
data = [
('A','C','D','F','G','I','M','P'), #each row here is a record, or a market basket with several items
('A','B','C','F','L','M','O'),
('B','F','H','J','O'),
('B','C','K','P'),
('A','C','E','F','L','M','N','P'),
('D','E','F','K','L','M','P'),
('A','C','F','L','M','P'),
('A','C','F','J','M','P'),
('B','G','J','K','O'),
('C','E','F','L','M','N','P')
]
///
}}}

You may change the data above and click on 'evaluate' if you want to try out the algorithm with a different sample dataset.

{{{id=6|
#auto
minsup = 0.6
partition_size = 5
///
}}}

You may choose different parameters by clicking above. Don't forget to click on 'evaluate' after you change them.

{{{id=3|
%hideall
#auto
def AprioriGen(F):
    C  = {}
    for x in F:
        for y in F:
            if len(x) == len(y): #in general case this may be required
                z = set(x)|set(y)
                if len(z) == len(x)+1: #true if x and y share all items except 1
                    prune = False
                    z = list(z)
                    z.sort()
                    for i in range(len(z)): #remove each item from z
                        item = z.pop(i)
                        if tuple(z) not in F: #is subset of z in F ?
                            prune = True
                            break
                        z.insert(i,item)
                    if not prune:
                        C[tuple(z)] = 0
    return C
///
}}}

{{{id=5|
%hide
#auto
def apriori(data, minsup):
    mincount = int(math.ceil(len(data) * minsup))
    print 'mincount =', mincount
    C = {}
    F = {}

    #--- first scan
    for r in data:
        r = list(r)
        r.sort()
        r = tuple(r)
        for i in r:
            if (i,) in C: C[(i,)] += 1
            else: C[(i,)] = 1

    Fk = dict((s,count) for s,count in C.iteritems() if count >= mincount)
    F.update(Fk)
    print '\nC1:-------------\n',C

    C = AprioriGen(Fk)
    print '\nF1:-------------\n',Fk
    print '\nC2:-------------\n',C

    #--- more scans
    k = 2
    while len(C) > 0:
        for r in data:
            sr = set(r)
            for s in C:
                if set(s) <= sr:
                    C[s] += 1

        Fk = dict((s,count) for s,count in C.iteritems() if count >= mincount)
        F.update(Fk)
        C = AprioriGen(Fk)
        print '\nF%d:-------------\n'%k,Fk
        print '\nC%d:-------------\n'%(k+1),C
        k += 1
    return F
///
}}}

Following is an implementation of the Partition algorithm. Clicking on 'evaluate' will show its output.

{{{id=12|
def partition():
    C = {} #global candidates
    
    #--- first scan: mine each partition
    for start in range(0,len(data),partition_size):
        print '\nPartition: %d:%d' % (start,start+partition_size)
        part = data[start : start+partition_size]
        F_local = apriori(part, minsup)
        C.update(F_local)
    
    #--- reset counters
    for i in C: C[i] = 0
    
    #--- second scan: get global counts
    for r in data:
        sr = set(r)
        for s in C:
            if set(s) <= sr:
                C[s] += 1

    mincount = int(math.ceil(len(data) * minsup))
    F = dict((s,count) for s,count in C.iteritems() if count >= mincount)
    return F
///
}}}

{{{id=4|
F = partition()
print '\nPartition Result:', F
///
Partition: 0:5
mincount = 3

C1:-------------
{('N',): 1, ('A',): 3, ('C',): 4, ('I',): 1, ('B',): 3, ('H',): 1, ('E',): 1, ('K',): 1, ('D',): 1, ('J',): 1, ('G',): 1, ('P',): 3, ('M',): 3, ('F',): 4, ('L',): 2, ('O',): 2}

F1:-------------
{('A',): 3, ('C',): 4, ('B',): 3, ('P',): 3, ('M',): 3, ('F',): 4}

C2:-------------
{('C', 'F'): 0, ('B', 'C'): 0, ('B', 'M'): 0, ('F', 'M'): 0, ('A', 'F'): 0, ('A', 'M'): 0, ('A', 'P'): 0, ('M', 'P'): 0, ('C', 'M'): 0, ('F', 'P'): 0, ('B', 'F'): 0, ('A', 'B'): 0, ('B', 'P'): 0, ('A', 'C'): 0, ('C', 'P'): 0}

F2:-------------
{('C', 'F'): 3, ('F', 'M'): 3, ('A', 'F'): 3, ('A', 'M'): 3, ('C', 'M'): 3, ('A', 'C'): 3, ('C', 'P'): 3}

C3:-------------
{('A', 'F', 'M'): 0, ('A', 'C', 'F'): 0, ('A', 'C', 'M'): 0, ('C', 'F', 'M'): 0}

F3:-------------
{('A', 'F', 'M'): 3, ('A', 'C', 'F'): 3, ('A', 'C', 'M'): 3, ('C', 'F', 'M'): 3}

C4:-------------
{('A', 'C', 'F', 'M'): 0}

F4:-------------
{('A', 'C', 'F', 'M'): 3}

C5:-------------
{}

Partition: 5:10
mincount = 3

C1:-------------
{('N',): 1, ('A',): 2, ('C',): 3, ('B',): 1, ('E',): 2, ('K',): 2, ('D',): 1, ('J',): 2, ('G',): 1, ('P',): 4, ('M',): 4, ('F',): 4, ('L',): 3, ('O',): 1}

F1:-------------
{('F',): 4, ('C',): 3, ('L',): 3, ('P',): 4, ('M',): 4}

C2:-------------
{('C', 'F'): 0, ('L', 'M'): 0, ('F', 'M'): 0, ('F', 'L'): 0, ('L', 'P'): 0, ('C', 'L'): 0, ('M', 'P'): 0, ('C', 'M'): 0, ('F', 'P'): 0, ('C', 'P'): 0}

F2:-------------
{('C', 'F'): 3, ('L', 'M'): 3, ('F', 'M'): 4, ('F', 'L'): 3, ('L', 'P'): 3, ('M', 'P'): 4, ('C', 'M'): 3, ('F', 'P'): 4, ('C', 'P'): 3}

C3:-------------
{('F', 'M', 'P'): 0, ('F', 'L', 'M'): 0, ('C', 'F', 'P'): 0, ('C', 'M', 'P'): 0, ('F', 'L', 'P'): 0, ('L', 'M', 'P'): 0, ('C', 'F', 'M'): 0}

F3:-------------
{('F', 'M', 'P'): 4, ('F', 'L', 'M'): 3, ('C', 'F', 'P'): 3, ('C', 'M', 'P'): 3, ('F', 'L', 'P'): 3, ('L', 'M', 'P'): 3, ('C', 'F', 'M'): 3}

C4:-------------
{('C', 'F', 'M', 'P'): 0, ('F', 'L', 'M', 'P'): 0}

F4:-------------
{('C', 'F', 'M', 'P'): 3, ('F', 'L', 'M', 'P'): 3}

C5:-------------
{}

Partition Result: {('C', 'F'): 6, ('C', 'P'): 6, ('F', 'M'): 7, ('F', 'M', 'P'): 6, ('F', 'P'): 6, ('C',): 7, ('M', 'P'): 6, ('C', 'M'): 6, ('P',): 7, ('M',): 7, ('F',): 8, ('C', 'F', 'M'): 6}
}}}

<h2 style="color: green;">Assignment</h2>
<ol>
<li> Define min-support and min-confidence with an example. </li>
<li> Find all frequent itemsets using apriori<br /><br /> 
<table border="1" cellspacing="0px" cellpadding="5px">
<tbody>
<tr>
<th>TID</th><th>items_bought</th>
</tr>
<tr>
<td>T100</td>
<td>{M, O, N, K, E, Y}</td>
</tr>
<tr>
<td>T200</td>
<td>{D, O, N, K, E, Y}</td>
</tr>
<tr>
<td>T300</td>
<td>{M, A, K, E}</td>
</tr>
<tr>
<td>T400</td>
<td>{M, U, C, K, Y}</td>
</tr>
<tr>
<td>T500</td>
<td>{C, O, O, K, I, E}</td>
</tr>
</tbody>
</table>
<br /> Minimum support = 60%, minimum confidence = 80%<br /><br /> </li>
<li> Compute the frequent itemsets for the same datasets for minimum support 30% and confidence 80%. Comment on the number  of rules output in both cases.<br /><br /> </li>
</ol>
<h2 style="color: green;">References</h2>
<ul>
<li>Ashok Savasere,                                                                                   Edward Omiecinski, Shamkant Navathe. An Efficient Algorithm for Mining Association Rules in Large Databases (VLDB 95), Pg. 432-444, Zurich, Switzerland, 1995.</li>
<li> R. Agarwal, T. Imielinski and A. Swami. Mining Association Rules between sets of items in large databases. In Proc. 1993 ACM SIGMOD Intl. Conf. Management of Data (SIGMOD 93), Pg. 207-216, Washington D.C., May 1993. </li>
</ul>

<br /><br />
For providing <b>Feedback</b> please click <a href="http://virtual-labs.ac.in/feedback/">here</a>.<br />

<br />
- Sponsored by MHRD: <a href="http://virtual-labs.ac.in/nmeict/" target="_blank">click</a>
- Licensing Terms: <a href="http://virtual-labs.ac.in/licensing/" target="_blank">click</a>

{{{id=11|

///
}}}