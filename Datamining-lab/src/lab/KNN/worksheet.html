

<center>
<img src="http://virtual-labs.ac.in/images/virtualLabsLogo.jpg" width="1220" alt="logo"/> 
</center>

<h1 style="color: green;">The k Nearest Neighbour Algorithm</h1>

<h2 style="color: green;">Introduction</h2>

<p>

Classification is a supervised data mining procedure in which individual items are placed into groups based on quantitative information on one or more characteristics inherent in the items (referred to as traits, variables, characters, etc) and based on a training set of previously labeled items. Classification can be thought of as two separate problems - binary classification and multiclass classification. A very familiar example of binary classification is the email spam-catching system: given a set of emails marked as spam and not-spam, it learns the characteristics of spam emails and is then able to process future email messages to mark them as spam or not-spam. It may be noted that the classes in this case are "spam" and "not-spam"

</p>



<h2 style="color: green;">Objective</h2>

<p>

Through this experiment we will explain classification using the kNN classification algorithm on a dummy 2 dimensional dataset.

</p>



<h2 style="color: green;">Theory</h2>

The k-nearest neighbours algorithm (k-NN) is a method for classifying objects based on closest training examples in the feature space.  The k-nearest neighbor algorithm is amongst the simplest of all machine learning  algorithms: an object is classified by a majority vote of its neighbors, with the object being assigned to the class most common amongst its k nearest neighbors.

<br />

<br />

Given a set of observations {(x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>) ..., (x<sub>n</sub>, y<sub>1</sub>)}, where each observation is a d-dimensional real vector with the class label, k-nearest neighbour algorithm attempts to assign a class label for an unseen test observation.

<br />

<pre>

Algorithm(Training Data, K, Test Sample)

{

	Neighbours = Find the closest K neighbours of TestSample.

	return Majority vote among neighbours.

}

</pre>



A variation of the standard k-nn, also known as weighted k-nn assigns weights to neighbours, where the weight of a neighbour is inversely proportional to the square of its distance from the tuple.



<h2 style="color: green;">Procedure</h2>



<br />

We present the kNN simulation in the next section. Some points to note are as follows:<br />



Given a dummy set of points and desired number of neighbours (<i>k</i> = 3), task is to find the class of the test tuple.

The class with label "green" is green in color and class with label "red" is red in color. The test tuple is white in color. The experiment proceeds as 

follows:

<ol>

<li>In the next step (click on the experiment to proceed), the distance of the test's tuple from all the points is measured and the edges from the test tuple to other points are drawn. The length of the edge is proportional to the euclidean distance between the test tuple and corresponding point.</li>



<li>In the next step (click to proceed), the edges colored as white denote the three desired number of nearest neighbour and the grey edges denote the other neighbours.</li>

<li>In the following step, only the three nearest neighbour's edges with the tuple are shown and the color of the test tuple is the class assigned to it <i>i.e.</i> red.</li>

<li>As the algorithm has halted, another click on the experiment takes it to the initial stage</li>

</ol>



<h2 style="color: green;">Simulation</h2>



Consider the following dataset:<br /><br /><br />

{{{id=2|
#auto

data = [
(1, 4.7, 50, 7), #each row here is a record with 4 attributes - rollno, height, weight, class
(2, 4.9, 52, 7),
(3, 5.2, 55, 7),
(4, 5.3, 54, 8),
(5, 5.2, 56, 8)
]
///
}}}

{{{id=21|

///
}}}

{{{id=19|

///
}}}

{{{id=17|

///
}}}

You may change the data above and click on 'evaluate' if you want to try out the algorithm with a different sample dataset.

{{{id=7|
#auto
test_record = (6, 5.2, 55)
///
}}}

You may choose a different test record by clicking above. Don't forget to click on 'evaluate' after you change the record. Next, is the parameter $k$ that is used for determining the number of nearest neighbours to consider.

{{{id=9|
#auto
k = 2
///
}}}

{{{id=16|

///
}}}

You may choose a different $k$ by clicking above.



Next, we define the distance function. In this case, it is merely computing the euclidean distance between 2 points (which are rows of the form given in the data above). You may change this to any other distance function of your choice.

{{{id=8|
#auto
def distance(x,y):
    return sqrt((x[1]-y[1])**2 + (x[2]-y[2])**2)
///
}}}

Finally, here is the code for the kNN algorithm. It computes the distances between the test record and all other records in the dataset, gets the nearest $k$ and outputs their majority class.

{{{id=5|
#auto
def kNN():
    print 'test record:', test_record
    #--- compute distances
    d = [(r,distance(test_record, r)) for r in data]
    d.sort(key=lambda i: i[1])
    #--- get majority class
    print 'nearest neighbours:'
    votes = {}
    for n in d[:k]: #enumerate k nearest neighbours
        print n
        c = n[0][3]
        if c in votes:
            votes[c] += 1
        else:
            votes[c] = 1    
    max_vote = -1
    max_class = None
    for c in votes:
        if max_vote < votes[c]:
            max_vote, max_class = votes[c], c
    print 'class =', max_class
///
}}}

{{{id=15|

///
}}}

This is just a call to the kNN function. Clicking on 'evaluate' in this cell will show the output of kNN.

{{{id=4|
kNN()
///
test record: (6, 5.20000000000000, 55)
nearest neighbours:
((3, 5.20000000000000, 55, 7), 0.000000000000000)
((5, 5.20000000000000, 56, 8), 1.00000000000000)
class = 8
}}}

{{{id=18|

///
}}}

{{{id=14|

///
}}}

<h2 style="color: green;">Assignment</h2>

<ol>

<li>

Write an algorithm for knn classification given  k and n, the number of attributes describing each tuple.

</li>

<li>

Use the classifier coded in the above example and examine the classification accuracy by:

<ol>

<li> Choosing different values of k

</li> 

<li>Assigning neighbours weights by distance.</li>

</ol>

</li>

</ol>



<h2 style="color: green;">References</h2>

				<ul>

				<li>

				E. Fix and J. L. Hodges Jr. Discriminatory analysis, non-parameteric discrimination: Consistency properties. In Technical report 21-49-004 (4), USAF school of aviation medicine, Randolph field, Texas - 1951.

				</li>

				<li>

				B. V. Dasarathy. Nearest neighbour (NN) Norms: NN pattern classification techniques. IEEE computer society press, 1991.

				</li>

				<li>



				R. O. Duda, P. E. Hart and D. G. Stork. Pattern Classification (2<sup>nd</sup> ed.). John Wiley and Sons, 2001.

				</li>

				<li>

				M. James. Classification algorithms. John Wiley and Sons, 1985.	

				</li>

				<li>

				D. Aha. Tolerating noise, irrelevant and novel attributes in instance based learning algorithms. Int. J. man-machine studies, 36: 267-287, 1992.

				</li>

				</ul>

<br /><br />
For providing <b>Feedback</b> please click <a href="http://virtual-labs.ac.in/feedback/">here</a>.<br />

<br />
- Sponsored by MHRD: <a href="http://virtual-labs.ac.in/nmeict/" target="_blank">click</a>
- Licensing Terms: <a href="http://virtual-labs.ac.in/licensing/" target="_blank">click</a>

{{{id=11|

///
}}}

{{{id=20|

///
}}}